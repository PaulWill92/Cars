{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autovillage webscraper\n",
    "\n",
    "Within this notebook i will be making a script that will crawl and scrape\n",
    "this [website](https://www.autovillage.co.uk/used-car/filter/bodystyle/saloon). <br>\n",
    "My goal is to create a data frame with the below features: <br>\n",
    "1. make - cars brand\n",
    "2. model - cars model\n",
    "3. doors - # of doors the car has\n",
    "4. body_type - saloon, hatchback, sport, suv, etc\n",
    "5. transmission - automatic or manual\n",
    "6. Mileage - # of miles on the odometer\n",
    "7. engine_size - in cc\n",
    "8. price - in £\n",
    "9. year - car's year of registration\n",
    "\n",
    "My plan of action is to parse into the elements I want on my web page, reverse engineer a for loop based off the containers I make, and use an array to store my data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The packages I will use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Webscraping libraries\n",
    "from urllib.request import urlopen # url inspector\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from selenium import webdriver # connects to chrome browser\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Web crawler imports\n",
    "import requests\n",
    "from requests import get\n",
    "\n",
    "# Web crawlers random seeds/time delays\n",
    "from time import sleep\n",
    "from random import randint\n",
    "\n",
    "# image viewer for cell outputs\n",
    "from IPython.display import display, Markdown, Latex, Image, display_html, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "##scrape a page with a fake browser\n",
    "# headers = {'User-Agent': 'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; Trident/4.0; Avant Browser; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0)'}\n",
    "# url = 'https://www.gumtree.com/cars/uk/page50'\n",
    "# web_page = requests.get(url,headers=headers)\n",
    "# print(web_page.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing a connection\n",
    "I will start by establishing a connection to the website autovilliage this will allow me to send and read responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish my url as a string\n",
    "my_url = 'https://www.autovillage.co.uk/used-car/filter/bodystyle/saloon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_client = urlopen(my_url) # open up a connection to the webpage\n",
    "autovillage_page =my_client.read() # reads all the html from the webpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing allows me to read the response object html syntax as a giant string which python can handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML parsing\n",
    "page_soup = BeautifulSoup(autovillage_page, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_soup.h1 # can view tags just by passing tag onto the soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traverse the HTML\n",
    "Capture exactly the elements I want by inspecting the web page code. Once I found the exact points within the HTML code I want to reference, I can save them as variables and reference them as containers in my for loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "page_soup.body # run this if you are a saddist and want to view the entire webpage html code or else use inspector on your web browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create container\n",
    "The container is the html code that houses exactly what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only focus on the html code that contains the info that's important to me\n",
    "# thats important for me\n",
    "container = page_soup.findAll(\"div\", {\"class\":\"ucatid20\"}) # found the ucatid20 tag from inspecting webpage and selecting the entire container="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many containers there are (should be 10 since 10 cars)\n",
    "len(container) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It found 10 of my cars on the page this is correct as I know visually there are 10 cars listed on each of the webpages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The images\n",
    "For fun I will webscrape the images and loop the outputs. This will be a good starting base to structure my web scraper on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will index into the html code that contains the images\n",
    "container[0].findAll(\"div\", {\"class\":\"mb5\"})[0].img\n",
    "# change first number index to swap cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds the images of the cars index into 1 car of the container and specify only the img flags within the containers html code\n",
    "image = container[0].findAll(\"div\", {\"class\":\"mb5\"})\n",
    "\n",
    "# use ipython display to view the html/yml code as an output cell\n",
    "display(HTML(str(image[0].img)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "great! the parsing worked now lets loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a for loop that iterates over all 10 containers and returns the images using ipython display output\n",
    "\n",
    "for item in range(0,len(container)):\n",
    "    display(HTML(str(container[item].findAll(\"div\", {\"class\":\"mb5\"})[0].img)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse containers\n",
    "Now that my little demo worked, I can parse into the features I will need for my Data Frame. I will still be using the alinkContainer as I found that the website seems to store its car details in that container.\n",
    "\n",
    "Just as above, I will test by parsing into 1 car container and once I get it right I will reverse engineer a for loop to iterate over all of the containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this container for our car names potentially\n",
    "name_container = page_soup.findAll(\"div\", {\"class\":\"alinkContainer\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is one path to get the name of the car but it might not be optimal\n",
    "name_container[0].div.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# price\n",
    "pricing = name_container[1].findAll(\"div\", {\"class\": \"avprice\"}) # our prices\n",
    "pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This container houses the price of the car's it is seperate from the other containers that house the details.\n",
    "pricing2 = name_container[1].find(\"div\", class_=\"avprice\") # our prices\n",
    "pricing2 # this method allows you to chain more find/findall commands without returning error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing into my features\n",
    "All of my features except price are in the same class:item flag. The only thing required is to index into the correct feature using python indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year + name \n",
    "year_name_model_html = container[0].div.findAll(\"div\", {\"class\":\"item\"})[0]\n",
    "year_name_model_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# engine + transmission\n",
    "eng_tran_html = container[0].div.span\n",
    "eng_tran_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# door + body type\n",
    "door_body_html = container[0].div.findAll(\"div\", {\"class\":\"item\"})[2].span\n",
    "door_body_html#.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mileage\n",
    "mileage_html = container[0].div.findAll(\"div\", {\"class\":\"item\"})[3].span\n",
    "mileage_html#.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! I found all of my features. The code above will help me create my scraper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looping my containers\n",
    "In this section, I will create for loops to append lists of elements from my cars container's and create data frame columns. I have already indexed into the right places I just need to iterate them. When I have got to the absolute end of my parsing, I can use the `get_text()` to extract the text. (use this only at the end). `.strip()` removes white space from the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container2 = page_soup.findAll(\"div\", {\"class\":\"avprice\"})\n",
    "container2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_url = \"https://www.autovillage.co.uk/used-car/seat/leon/mtz_30003_56226887\"\n",
    "html = urlopen(sub_url)\n",
    "subpage = html.read()\n",
    "sub_soup = BeautifulSoup(subpage, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_soup.findAll(\"div\", {\"class\":\"fl\"})[10].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/used-car/seat/leon/mtz_30003_56226887',\n",
       " '/used-car/mercedes-benz/a-class/mtz_91496_55745795',\n",
       " '/used-car/ford/focus/hcp_d-d8e29748_1a5a6e34',\n",
       " '/used-car/volkswagen/golf/mtz_40058_56380258',\n",
       " '/used-car/kia/venga/hcp_d-c927fd38_49ca42c4',\n",
       " '/used-car/vauxhall/astra/hcp_d-ee247a08_10b95eb1',\n",
       " '/used-car/suzuki/swift/mtz_23589_56336365',\n",
       " '/used-car/ford/fiesta/mtz_77936_56314739',\n",
       " '/used-car/mercedes-benz/a-class/hcp_d-a3187c55_db497e81',\n",
       " '/used-car/renault/clio/hcp_d-64efdffe_c63c2b71']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_url = []\n",
    "url_extensions = []\n",
    "car_color = []\n",
    "fuel = []\n",
    "for i in range(0,1): #range of pages to scrape\n",
    "    url= 'https://www.autovillage.co.uk/used-car/page/{}/filter/bodystyle/hatchback'.format(i)\n",
    "    html= urlopen(url)\n",
    "    autovillage_page= html.read()\n",
    "    soup= BeautifulSoup(autovillage_page, \"html.parser\")\n",
    "    \n",
    "    # Define my containers so I can reference them in below loops\n",
    "    container= soup.findAll(\"div\", {\"class\":\"ucatid20\"})\n",
    "    container2= soup.findAll(\"div\", {\"class\":\"avprice\"})\n",
    "\n",
    "    # iterate into my price container\n",
    "\n",
    "\n",
    "\n",
    "    url_container = soup.findAll(\"div\", {\"class\":\"alink\"})\n",
    "\n",
    "\n",
    "    for a in url_container:\n",
    "        url_extensions.append(a.a['href'])\n",
    "\n",
    "url_extensions\n",
    "#     for url in url_extensions:\n",
    "#         sub_url.append(\"https://www.autovillage.co.uk{}\".format(url))\n",
    "\n",
    "#         for item in sub_url:\n",
    "#             html = urlopen(item)\n",
    "#             subpage = html.read()\n",
    "#             soup_sub = BeautifulSoup(subpage, \"html.parser\")\n",
    "            \n",
    "#             container3 = soup_sub.findAll(\"div\", {\"class\":\"fl\"})[10].get_text()\n",
    "            \n",
    "#             for item in range(0,len(container3)):\n",
    "            \n",
    "#                 car_color.append(container3[item])\n",
    "# #             fuel.append(soup_sub.findAll(\"div\", {\"class\":\"fl\"})[8].get_text())\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price =[] # car price\n",
    "year_make_model =[] # year made, brand name, model\n",
    "eng_tran =[] # engine size and transmission type\n",
    "door_body =[] # number of doors and body style\n",
    "mileage =[] # number of miles on the odometer\n",
    "\n",
    "# loop within container2 and return just the text\n",
    "for item in container2:\n",
    "    \n",
    "    #price\n",
    "    price.append(item.text)\n",
    "    \n",
    "for item in range(0,len(container)):\n",
    "    \n",
    "    #year, make, and model\n",
    "    car_names= container[item].div.findAll(\"div\", {\"class\":\"item\"})[0]\n",
    "    year_make_model.append(car_names.get_text().strip())\n",
    "    \n",
    "    #engine size and transmission type\n",
    "    tran = container[item].div.span\n",
    "    eng_tran.append(tran.get_text())\n",
    "    \n",
    "    # number of doors and car body type\n",
    "    door_bod = container[item].div.findAll(\"div\", {\"class\":\"item\"})[2].span\n",
    "    door_body.append(door_bod.get_text())\n",
    "    \n",
    "    # Car mileage\n",
    "    car_mileage = container[item].div.findAll(\"div\", {\"class\":\"item\"})[3].span\n",
    "    mileage.append(car_mileage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if the features turned out alright\n",
    "price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets count how many cars we have in our features it should be 10\n",
    "print(\"Rows in price:\",len(price))\n",
    "print(\"Rows in mileage:\",len(mileage))\n",
    "print(\"Rows in door count/body style:\",len(door_body))\n",
    "print(\"Rows in engine size/transmission:\",len(eng_tran))\n",
    "print(\"Rows in year/make/model:\",len(year_make_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Our data frame\n",
    "Test our webscraper by making a data frame practice run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_df = pd.DataFrame({'price':price, \n",
    "                       'mileage':mileage, \n",
    "                       'door/body':door_body, \n",
    "                       'eng/tran':eng_tran, \n",
    "                       'year/make/model':year_make_model})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! the webscraper worked! but this is only effective for 1 page. We need to set up a webcrawler to do many other pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save DF\n",
    "Testing save for DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '../Raw-Data/car_practice.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_df.to_csv(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating My Webcrawler\n",
    "Now that I know everything works on each webpage, I will automate the process of scraping my web pages by creating a webcrawler.\n",
    "\n",
    "A crawler essentially goes through each page and applies your web scraper. It's very handy as I plan on using a few thousand cars for my data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up crawler\n",
    "\n",
    "# this line of code takes my first webpage and iterates into my defined page range\n",
    "for i in range(0,1): #range of pages to scrape\n",
    "    url= 'https://www.autovillage.co.uk/used-car/page/{}/filter/bodystyle/saloon'.format(i)\n",
    "    html= urlopen(url)\n",
    "    autovillage_page= html.read()\n",
    "    soup= BeautifulSoup(autovillage_page, \"html.parser\")\n",
    "    \n",
    "    # Define my containers so I can reference them in below loops\n",
    "    container= soup.findAll(\"div\", {\"class\":\"ucatid20\"})\n",
    "    container2= soup.findAll(\"div\", {\"class\":\"avprice\"})\n",
    "\n",
    "    # iterate into my price container\n",
    "    for item in container2:\n",
    "        # create price feature\n",
    "        price.append(item.text)\n",
    "        \n",
    "    # iterate into my other features container   \n",
    "    for item in range(0,len(container)):\n",
    "    \n",
    "        #year, make, and model\n",
    "        car_names= container[item].div.findAll(\"div\", {\"class\":\"item\"})[0]\n",
    "        year_make_model.append(car_names.get_text().strip())\n",
    "    \n",
    "        #engine size and transmission type\n",
    "        tran = container[item].div.span\n",
    "        eng_tran.append(tran.get_text())\n",
    "    \n",
    "        # number of doors and car body type\n",
    "        door_bod = container[item].div.findAll(\"div\", {\"class\":\"item\"})[2].span\n",
    "        door_body.append(door_bod.get_text())\n",
    "    \n",
    "        # Car mileage\n",
    "        car_mileage = container[item].div.findAll(\"div\", {\"class\":\"item\"})[3].span\n",
    "        mileage.append(car_mileage)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! the webcrawler worked! Now lets form a data frame out of the arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFrame\n",
    "car_df_main = pd.DataFrame({'price':price, \n",
    "                       'mileage':mileage, \n",
    "                       'door/body':door_body, \n",
    "                       'eng/tran':eng_tran, \n",
    "                       'year/make/model':year_make_model})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_df_main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked! we created our data frame and we can run this script as many times we want to retrieve more cars in the future. This website refreshes it's cars order and I didn't set a random seed so I am unable to replicate this exact dataframe more than once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Data Frame Save\n",
    "I will save my main created dataframe as a csv file in my raw data folder. I will then load it into my [Data Cleaning](https://github.com/PaulWill92/cars/blob/master/Jupyter-Notebooks/02-Data_Cleaning.ipynb) notebook and proceed to clean it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our save directory path\n",
    "car_save_path = '../Raw-Data/new-saloon.csv'\n",
    "car_df_main.to_csv(car_save_path) # running this cell multiple times overwrites saves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
